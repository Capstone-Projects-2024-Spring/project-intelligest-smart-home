---
sidebar_position: 5
---

# Use-case descriptions

## Light Management Case

An IntelliGest Home user is looking to switch lights on or off in different rooms using specific ASL/Gestures.
1. The user makes sure to add the integration for the type of bulbs they have.
2. The user connects their device to IntelliGest Home service.
3. The user performs a specific gesture designated for light control.
4.	Once the system detects the gesture, IntelliGest processes the input and provides immediate visual feedback.
5.	Then, the light has been successfully turned on or off in different rooms. 

## Weather Information Case

On a Rainy day, a Deaf person is sitting on his couch in the basement and wondering if the weather is still bad weather.
1.	The user does certain gestures in front of the IntelliGest Camera.
2.	The user sees a certain light strip color and knows IntelliGest caught the Gesture.
3.	Then, the User sees a certain color and knows it is processing the request.
4.	Finally, the User sees on the display the Weather for the rest of the day.

## Light Color Case

A Deaf person is cooking dinner in the kitchen and wants to change the color of the kitchen’s light from brown to yellow
1.	The user does certain Gestures in front of the IntelliGest Camera.
2.	The user sees a certain light strip color and knows IntelliGest caught the Gesture.
3.	Then, the User sees a certain color and knows it is processing the request.
4.	Finally, the User sees the kitchen light is changed to yellow by IntelliGest.


## Smart Lock Management Case

A brand new user is trying to set up a smart lock in their home to control.
1.	The user sets up their smart lock device.
2.	The user accesses their dashboard display.
3.	The user sets up and connects the smart lock device to IntelliGest.
4.	The smart lock should appear on the dashboard as either locked or unlocked.
5.	The user can now perform the action mapped to unlocking and locking.

## News Case

A Computer Science graduate Deaf person wants to add a new gesture to know the latest scores of their favorite NFL.

1.	User adds his Gesture to Google’s Teachable Machine.
2.	Links that Gesture to know the latest NFL scores. 
3.	To test, the User performs that gesture in front of the IntelliGest Camera.
4.	The user sees a certain light color to know it is processing the request.
5.	Finally, they see the latest NFL scores on the display.

## Notes/Reminders Case

A Deaf person wants to add ‘Buy a couple of Milk Gallons’ in his Reminders.

1.	The user does certain Gestures in front of IntelliGest.
2.	The user sees certain lights and knows IntelliGest caught that request.
3.	The user sees certain lights and knows his request is processing.
4.	The user sees IntelliGest is asking for confirmation to make sure it correctly recognizes the Gesture.
5.	The User confirms it.
6.	The user sees on the Display screen that ‘Buy a couple of Milk Gallons’ is added to his Mobile’s Reminders.
7.	Whenever he opens his Mobile Device, he will see ‘Buy a couple of Milk Gallons’ in his Reminders.

## Temperature Management Case

A user wants to adjust their home temperature.
1. The user starts the temperature adjustment process by performing ASL signs for hotter or colder.
2. Once detected the user gesture, IntelliGest provides feedback to acknowledge the user
3. IntellGest processes the temperature request. 
4. The user saw the screen displaying another color which means the commands are being processed.
5. Finally, the temperature has been adjusted and the system provides visual confirmation on the display .

## Verbal and Non-Deaf Case

A verbal non-Deaf person wants to use IntelliGest 
1.	User goes to his Account Settings and changes it for verbal non-Deaf people. 
2.	Then, the user says “Hey IntelliGest what’s the Top News”.
3.	The user sees certain lights and knows IntelliGest has received the request.
4.	The user sees certain lights and knows IntelliGest is processing his request.
5.	Finally, IntelliGest goes “U.S. intends more airstrikes on Iran-backed targets, NSA Jake Sullivan tells Kristen Welker”, responding to the question.

## Custom Gesture Case

A non-verbal Deaf person wants to use IntelliGest to create custom gestures that are overlooked/complex 
1.	User tries a gesture that isn’t included in recognition. 
2.	User selects the “custom gesture” option and names the gesture. 
3.	Device produces on-screen instructions for User to perform gesture. 
4.	User’s gesture is mapped to his selected action/combination of actions. 
5.	Gesture is saved and can now be used at any time until disabled. 



